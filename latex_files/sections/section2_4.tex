\subsection{Preconditioner for Linear Systems}
As we discussed stationary iterative methods in \autoref{sec:iterative-methods}, we now review these methods from a preconditioning perspective.

The Jacobi and Gauss-Seidel iterations are of the form
\begin{equation}
    \boldsymbol{u}^{k+1} = \boldsymbol{Q}\boldsymbol{u}^{k} + \boldsymbol{q}
    \label{eqn:iterative-form}
\end{equation}

in which
\begin{align}
    \boldsymbol{Q}_{JA} &= \boldsymbol{I} - \boldsymbol{D}^{-1}\boldsymbol{A} \\
    \boldsymbol{Q}_{GS} &= \boldsymbol{I} - {\boldsymbol{D} - \boldsymbol{L}}^{-1}\boldsymbol{A}
\end{align}

for the Jacobi and Gauss-Seidel iterations, respectively. Given the matrix splitting
\begin{equation}
    \boldsymbol{A} = \boldsymbol{S} - (\boldsymbol{S} - \boldsymbol{A})
\end{equation}
a \textit{linear fixed-point iteration} can be defined by the recurrence
\begin{equation}
    \boldsymbol{u}^{k+1} = \boldsymbol{S}^{-1}(\boldsymbol{S} - \boldsymbol{A})\boldsymbol{u}^k + \boldsymbol{S}^{-1}\boldsymbol{f}
\end{equation}
which has the form \autoref{eqn:iterative-form} with 
\begin{equation}
    \boldsymbol{Q} = \boldsymbol{S}^{-1}(\boldsymbol{S} - \boldsymbol{A}) = \boldsymbol{I} - \boldsymbol{S}^{-1}\boldsymbol{A},\quad \boldsymbol{q} = \boldsymbol{S}^{-1}\boldsymbol{f}
    \label{eqn:definition-Q-q}
\end{equation}

For example, for the Jacobi iteration, $\boldsymbol{S} = \boldsymbol{D}, \boldsymbol{S} - \boldsymbol{A} = \boldsymbol{D} - \boldsymbol{A}$, while for the Gauss-Seidel iteration $\boldsymbol{S} = \boldsymbol{D} - \boldsymbol{L}, \boldsymbol{S} - \boldsymbol{A} = \boldsymbol{D} - \boldsymbol{L} - \boldsymbol{A} = \boldsymbol{U}$.


The iteration $\boldsymbol{u}^{k+1} = \boldsymbol{Q}\boldsymbol{u}^k + \boldsymbol{q}$ can also be viewed as a technique for solving the system
\begin{equation}
    (\boldsymbol{I} - \boldsymbol{Q})\boldsymbol{u} = \boldsymbol{q}
\end{equation}
Since $\boldsymbol{Q}$ has the form $\boldsymbol{Q} = \boldsymbol{I} - \boldsymbol{S}^{-1}\boldsymbol{A}$, this system can be rewritten as
\begin{equation}
    \boldsymbol{S}^{-1}\boldsymbol{A}\boldsymbol{u} = \boldsymbol{S}^{-1}\boldsymbol{f}
    \label{eqn:preconditioned-system}
\end{equation}
We call this system that has the same solution as the original system $\boldsymbol{A}\boldsymbol{u} = \boldsymbol{f}$ the \textit{preconditioned system} and $\boldsymbol{S}$ the preconditioning matrix or preconditioner. It's often to use $\boldsymbol{M}$ to denote $\boldsymbol{S}$ when the splitting matrix $\boldsymbol{S}$ is used in the preconditioning. In other words, a \textit{relaxation scheme} is equivalent to a \textit{fixed-point iteration} on a \textit{preconditioned system}.
The preconditioning matrices can be easily derived for the Jacobi, Gauss-Seidel, SOR and SSOR iterations as follows
\begin{align}
    \boldsymbol{M}_{JA} &= \boldsymbol{D} \\
    \boldsymbol{M}_{GS} &= \boldsymbol{D} - \boldsymbol{L} \\
    \boldsymbol{M}_{SOR} &= \frac{1}{\omega}(\boldsymbol{D} - \omega\boldsymbol{L}) \\
    \boldsymbol{M}_{SSOR} &= \frac{1}{\omega(2-\omega)}(\boldsymbol{D} - \omega\boldsymbol{L})\boldsymbol{D}^{-1}(\boldsymbol{D} - \omega\boldsymbol{U})
\end{align}


% A linear system $\boldsymbol{A} \boldsymbol{u} = \boldsymbol{f}$ can be preconditioned by a matrix $\boldsymbol{H}^{-1}$ from the left as follows

% \begin{equation}
%     \boldsymbol{H}^{-1}\boldsymbol{A} \boldsymbol{u} =  \boldsymbol{H}^{-1}\boldsymbol{f}
%     \label{eqn:preconditioned_system}
% \end{equation}

The matrix $\boldsymbol{M}^{-1}$ should be symmetric and positive definite. Even though $\boldsymbol{M}$ is sparse most of the time, there is no guarantee that the $\boldsymbol{M}^{-1}$ is sparse. And this limits the number of techniques that can be applied to solve the preconditioned system. Also the computation of $\boldsymbol{M}^{-1}\boldsymbol{b}$ for any vector $\boldsymbol{b}$ so the actual solution to the problem can be easily obtained from the solution to the preconditioned system.


\subsection{General Convergence Results}
\label{section:convergence-analysis}
In this section, we examine the convergence behaviors of the general preconditioners above. The detailed analysis can be found in \cite{doi:10.1137/1.9780898718003}. We choose the most important results to present here with notations adapted to match the other sections of the paper.
All methods seen in the previous section define a sequence of iterates of the form
\begin{equation}
    \boldsymbol{u}^{k+1} = \boldsymbol{Q}\boldsymbol{u}^k + \boldsymbol{q}
    \label{eqn:iter-eqn-1}
\end{equation}
where $\boldsymbol{Q}$ is the iteration matrix. We need to answer these two questions
\begin{itemize}
    \item If the iteration converges, is the limit indeed a solution of the original system?
    \item Under which conditions does the iteration converge?
    \item How fast is the convergence?
\end{itemize}
If the above iteration converges to $\boldsymbol{u}$, and it satisfies
\begin{equation}
    \boldsymbol{u} = \boldsymbol{Q}\boldsymbol{u} + \boldsymbol{q}
    \label{eqn:iter-eqn-end}
\end{equation}
Recall the definition in \autoref{eqn:definition-Q-q}, it's easy to verify the $\boldsymbol{u}$ satisfies $\boldsymbol{A}\boldsymbol{u}= \boldsymbol{f}$. This answers the first question. We now consider the next two questions.

If $\boldsymbol{I} - \boldsymbol{Q}$ is nonsingular, then there is a solution $\boldsymbol{u}^*$ to the equation \autoref{eqn:iter-eqn-end}. Substracting \autoref{eqn:iter-eqn-end} from \autoref{eqn:iter-eqn-1} yields
\begin{equation}
    \boldsymbol{u}^{k+1} - \boldsymbol{u}^* = \boldsymbol{Q}(\boldsymbol{u}^k - \boldsymbol{u}^*) = \cdots \boldsymbol{Q}^{k+1}(\boldsymbol{u}^0 - \boldsymbol{u}^*)
\end{equation}



If the spectral radius of the iteration matrix $\boldsymbol{Q}$ is less than 1, then $\boldsymbol{u}^k - \boldsymbol{u}^0$ converges to zero. And the iteration \autoref{eqn:iter-eqn-1} converges toward the solution defined by \autoref{eqn:iter-eqn-end}. Conversely, the relation
\begin{equation}
    \boldsymbol{u}^{k+1} - \boldsymbol{u}^k = \boldsymbol{Q}(\boldsymbol{u}^{k} - \boldsymbol{u}^{k-1}) = \cdots \boldsymbol{Q}^{k} (\boldsymbol{q} - (\boldsymbol{I} - \boldsymbol{Q})\boldsymbol{u}^0))
\end{equation}

shows that if the iteration converges for any $\boldsymbol{u}^0$ and $\boldsymbol{q}$, then $\boldsymbol{Q}^k\boldsymbol{b}$ converges to zero for any vector $\boldsymbol{b}$. As a result, $\rho(\boldsymbol{Q})$ must be less than 1. The following theorem is proved:
\begin{theorem}
Let $\boldsymbol{Q}$ be a square matrix such that $\rho(\boldsymbol{Q}) < 1$. Then $\boldsymbol{I} - \boldsymbol{Q}$ is non-singular and iteration \autoref{eqn:iter-eqn-1} converges for any $
\boldsymbol{q}$ and $\boldsymbol{u}^0$. Conversely, if the iteration \autoref{eqn:iter-eqn-1} converges for any $\boldsymbol{q}$ and $\boldsymbol{u}^0$, then $\rho(\boldsymbol{Q}) < 1$
\end{theorem}
The theorem and its proof can be found in \cite{doi:10.1137/1.9780898718003}
Since it is often expensive to compute the spectral radius of a matrix, sufficient conditions that guarantee convergence can be useful in practice. One such sufficient condition could be obtained by utilizing the inequality $\rho(\boldsymbol{Q} \leq ||\boldsymbol{Q}||)$ for any matrix norm \cite{doi:10.1137/1.9780898718003}.
\begin{corollary}
Let $\boldsymbol{Q}$ be a square matrix such that $||\boldsymbol{Q}|| < 1$  for some matrix norm $||\cdot||_\cdot$. Then $\boldsymbol{I} - \boldsymbol{Q}$ is non-singular and the iteration \autoref{eqn:iter-eqn-1} converges for any initial vector $\boldsymbol{u}^0$
\end{corollary}

Other than knowing that \autoref{eqn:iter-eqn-1} converges, we can also know how fast it converges. The error $\boldsymbol{e}^k = \boldsymbol{u}^k - \boldsymbol{u}^*$ at step $k$ satisfies that 
\begin{equation}
    \boldsymbol{e}^k = \boldsymbol{Q}^k \boldsymbol{e}^0
\end{equation}
The proof can be found in \cite{doi:10.1137/1.9780898718003}

% We can prove this by expressing $\boldsymbol{Q}$ in Jordan canonical form
% \begin{equation}
%     \boldsymbol{Q} = \boldsymbol{X}\boldsymbol{J}\boldsymbol{X}^{-1}
% \end{equation}
% For simplicity, we assume that there is only one eigenvalue of $\boldsymbol{Q}$ of the largest modulus denoted by $\lambda$. Then
% \begin{equation}
%     \boldsymbol{e}^k = \lambda^k\boldsymbol{X}(\frac{\boldsymbol{J}}{\lambda})^k\boldsymbol{X}^{-1}\boldsymbol{e}^0    
% \end{equation}
% The matrix $\boldsymbol{J}/\lambda$ shows that all its blocks, except the block associated with the eigenvalue $\lambda$, converge to zero as $k \to \infty$. Let this Jordan block be of size $p$, and of the form $\boldsymbol{J}_\lambda = \lambda \boldsymbol{I} + \boldsymbol{L}$. Here $\boldsymbol{L}$ is nilpotent of $p$, i.e. $\boldsymbol{L}^p = 0$. Then for $k \geq p$
% \begin{equation}
%     \boldsymbol{J}_\lambda^k =  (\lambda \boldsymbol{I} + \boldsymbol{L})^k = \lambda^k(\boldsymbol{I} + \lambda^{-1}\boldsymbol{L})^{k} = \lambda^k(\sum_{i=0}^{p-1}{\lambda^{-i}{k \choose i}\boldsymbol{L}^i})
% \end{equation}

% If $k$ is large enough, then for any $\lambda$ the dominant term in the above sum is the last term, i.e.,
% \begin{equation}
%     \boldsymbol{J}_\lambda^k \approx \lambda^{k-p+1}{k \choose p-1}\boldsymbol{L}^{p-1}
% \end{equation}
% Thus, the norm of $\boldsymbol{e}^k = \boldsymbol{Q}^k\boldsymbol{e}^0$ has the asymptotical form
% \begin{equation}
%     ||\boldsymbol{e}^k|| \approx C \times |\lambda^{k-p+1}|{k \choose p-1}
% \end{equation}
% where C is some constant. The \textit{convergence factor} of a sequence is the limit
% \begin{equation}
%     \rho = \lim_{k\to\infty}(\frac{||\boldsymbol{e}^k||}{||\boldsymbol{e}^0||})^{1/k}
% \end{equation}
% From above analysis, $\rho = \rho(\boldsymbol{Q})$. The \textit{convergence rate} $\tau$ is the natural logarithm of the inverse of the convergence factor
% \begin{equation}
%     \tau = - \ln \rho
% \end{equation}
% Note that the above definition depends on the initial vector $\boldsymbol{e}^0$, so it maybe be termed a \textit{specific} convergence factor. A \textit{general} convergence factor can be defined by
% \begin{equation}
%     \phi = \lim_{k\to\infty}(\max_{\boldsymbol{u}^0\in\mathbb{R}^n}\frac{||\boldsymbol{e}^k||}{||\boldsymbol{e}^0||})^{1/k}
% \end{equation}
% This satisfies
% \begin{align}
% \phi &= \lim_{k\to\infty}(\max_{\boldsymbol{u}^0\in\mathbb{R}^n}\frac{||\boldsymbol{Q}^k\boldsymbol{e}^0||}{||\boldsymbol{e}^0||})^{1/k} \\
% &= \lim_{k\to\infty}(||\boldsymbol{Q}^k||)^{1/k} = \rho(\boldsymbol{Q})
% \end{align}
% Thus, 
The global asymptotic convergence factor is equal to the spectral radius of the iteration matrix. 
The general convergence rate differs from the specific rate only when the initial error does not have any components in the invariant subspace associated with the dominant eigenvalues. Since it is hard to know a priori, the general convergence factor is more useful in practice. The above analysis can be found in \cite{doi:10.1137/1.9780898718003}.

Using convergence analysis here, the convergence criteria for several iterative methods such as Richardson's Iteration, and regular splitting can be derived with simpler forms. One type of matrices that is worth notice is diagonally dominant matrices. We begin with a few standard definitions
\begin{definition}
    A matrix A is 
    \begin{itemize}
        \item (weakly) diagonally dominant if \begin{equation}
            |a_{j,j}| \geq \sum_{i=1,i\neq j}^{i=n}|a_{i,j}|,\quad j = 1,\dots,n
        \end{equation}
        \item strictly diagonally dominant if A is irreducible, and \begin{equation}
             |a_{j,j}| > \sum_{i=1,i\neq j}^{i=n}|a_{i,j}|,\quad j = 1,\dots,n
        \end{equation}
        \item irreducibly diagonally dominant if \begin{equation}
             |a_{j,j}| \geq \sum_{i=1,i\neq j}^{i=n}|a_{i,j}|,\quad j = 1,\dots,n
        \end{equation} 
        with strict inequality for at least one $j$
    \end{itemize}
\end{definition}

The diagonally dominant matrices are important as many matrices from the discretization of PDEs are diagonally dominant. When solving these linear systems with iterative methods, the spectral radius can be estimated using Gershgorin's theorem. Gershgorin's theorem allows rough locations for all eigenvalues of $\boldsymbol{A}$ to be determined. In situations where $\boldsymbol{A}$ is so large that the eigenvalues of $\boldsymbol{A}$ are unable to obtain, for example, a linear system from extremely fine discretization, the spectral radius can be directly obtained via the entries of the matrix $\boldsymbol{A}$. The simplest such result is the bound \begin{equation}
    |\lambda_i| \leq ||\boldsymbol{A}||
\end{equation}
for any matrix norm. Gershgorin's theorem provides a more precise localization result
\begin{theorem}[Gershgorin]
Any eigenvalue $\lambda$ of a matrix $\boldsymbol{A}$ is located in one of the closed discs of the complex plane centered at $a_{i, i}$ and has the radius
\begin{equation}
    \rho_i = \sum_{j = 1, j \neq i}^{j=n}|a_{i,j}|
\end{equation}
In other words,
\begin{equation}
    \forall \lambda \in \sigma(\boldsymbol{A}), \exists  i \text{ such that} |\lambda - a_{i,i}| \leq \sum_{j = 1, j \neq i}^{j = n}|a_{i,j}|
\end{equation}
\end{theorem}
The theorem and its proof can be found in \cite{doi:10.1137/1.9780898718003}
% \begin{proof}
% Let $x$ be an eigenvector associated with an eigenvalue $\lambda$, and let $m$ be an index of the component of largest modulus in $\boldsymbol{x}$, scale $\boldsymbol{x}$ so that $|\xi_m| = 1$, and $\xi_i \leq 1, \forall i \neq m$. Since $\boldsymbol{x}$ is an eigenvector, then
% \begin{equation}
%     (\lambda - a_{m,m})\xi_m = - \sum_{j = 1, j \neq m}^{n}a_{m,j}\xi_j
% \end{equation}
% which gives
% \begin{equation}
%     |\lambda - a_{m,m}| \leq \sum_{j = 1, j \neq m}^{n}|a_{m,j}||\xi_j| \leq \sum_{j = 1, j \neq m}^{n}|a_{m,j}| = \rho_m
% \end{equation}
% \end{proof}
This result also holds for the transpose of $\boldsymbol{A}$, so this theorem can also be formulated either based on column sums or row sums.
The $n$ discs defined in the theorem are called Gershgorin discs. The theorem states that the union of these n discs contains the spectrum of $\boldsymbol{A}$. It can also be shown that if there are $m$ Gershgorin discs whose union $S$ is disjoint from all other discs, then $S$ contains exactly $m$ eigenvalues (with multiplicities counted). 
Additional refinement which has important consequences concerns of a particular case when $\boldsymbol{A}$ is irreducible is given here.
\begin{theorem}
Let $\boldsymbol{A}$ be an irreducible matrix and assume that an eigenvalue $\lambda$ of $\boldsymbol{A}$ lies on the boundary of the union of the $n$ Gershgorin discs. Then $\lambda$ lies on the boundary of all Gershgorin discs
\end{theorem}
This theorem and its proof can be found in \cite{doi:10.1137/1.9780898718003} where an immediate corollary of the Gershgorin theorem and this theorem follows
\begin{corollary}
If a matrix $\boldsymbol{A}$ is strictly diagonally dominant or irreducibly diagonally dominant, then it is nonsingular.
\end{corollary}
This leads to the following theorem
\begin{theorem}
If $\boldsymbol{A}$ is a strictly diagonally dominant or an irreducibly diagonally dominant matrix, then the associated Jacobi and Gauss-Seidel iterations converge for any $\boldsymbol{u}^0$.
\end{theorem}
The convergence condition for SPD matrices is given as follows
\begin{theorem}
if $\boldsymbol{A}$ is symmetric with positive diagonal elements and for $0 < \omega < 2$, SOR converges for any $\boldsymbol{u}^0$ if and only if $\boldsymbol{A}$ is positive definite. 
\end{theorem}
These theorems for convergence play an important role in the study of various preconditioners and can be found in \cite{doi:10.1137/1.9780898718003}.
More iterative methods and preconditioners as well as their convergence criteria can be found in \cite{doi:10.1137/1.9780898718003}.